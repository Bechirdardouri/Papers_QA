{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10337673,"sourceType":"datasetVersion","datasetId":6401265},{"sourceId":11796267,"sourceType":"datasetVersion","datasetId":7407516}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:48:06.513752Z","iopub.execute_input":"2025-05-14T18:48:06.514056Z","iopub.status.idle":"2025-05-14T18:48:10.054133Z","shell.execute_reply.started":"2025-05-14T18:48:06.514034Z","shell.execute_reply":"2025-05-14T18:48:10.053325Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -U bitsandbytes -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:48:11.316569Z","iopub.execute_input":"2025-05-14T18:48:11.316879Z","iopub.status.idle":"2025-05-14T18:49:33.360170Z","shell.execute_reply.started":"2025-05-14T18:48:11.316849Z","shell.execute_reply":"2025-05-14T18:49:33.359118Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_API_KEY\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:49:36.301419Z","iopub.execute_input":"2025-05-14T18:49:36.302514Z","iopub.status.idle":"2025-05-14T18:49:36.533036Z","shell.execute_reply.started":"2025-05-14T18:49:36.302477Z","shell.execute_reply":"2025-05-14T18:49:36.532135Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:49:37.482300Z","iopub.execute_input":"2025-05-14T18:49:37.482599Z","iopub.status.idle":"2025-05-14T18:49:37.491808Z","shell.execute_reply.started":"2025-05-14T18:49:37.482579Z","shell.execute_reply":"2025-05-14T18:49:37.491132Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"login(secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:49:38.363831Z","iopub.execute_input":"2025-05-14T18:49:38.364415Z","iopub.status.idle":"2025-05-14T18:49:38.482336Z","shell.execute_reply.started":"2025-05-14T18:49:38.364391Z","shell.execute_reply":"2025-05-14T18:49:38.481714Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:49:40.493323Z","iopub.execute_input":"2025-05-14T18:49:40.493607Z","iopub.status.idle":"2025-05-14T18:49:44.974249Z","shell.execute_reply.started":"2025-05-14T18:49:40.493587Z","shell.execute_reply":"2025-05-14T18:49:44.973063Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# 1. Imports and Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport glob\nimport json\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nimport evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Loading and Extracting Data from JSON Papers","metadata":{}},{"cell_type":"code","source":"\ndef load_papers(directory=\"/kaggle/input/assignementdataset\"):\n    json_files = [f for f in Path(directory).glob(\"*.json\") if f.is_file()]\n    all_data = []\n    for file in tqdm(json_files, desc=\"Loading JSON files\"):\n        with open(file, 'r') as f:\n            try:\n                doc = json.load(f)\n                text = extract_text_from_json(doc)\n                all_data.append({\"file\": file.name, \"text\": text})\n            except json.JSONDecodeError as e:\n                print(f\"Error parsing {file.name}: {e}\")\n                continue\n    return pd.DataFrame(all_data)\n\ndef extract_text_from_json(doc):\n    if \"body_text\" in doc:\n        return \" \".join([section[\"text\"] for section in doc[\"body_text\"] if \"text\" in section])\n    elif isinstance(doc, dict):\n        return \" \".join(str(v) for v in doc.values() if isinstance(v, str))\n    return \"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Splitting the QA Dataset into Training and Testing","metadata":{}},{"cell_type":"code","source":"def split_qa_dataset(qa_df, test_size=0.2, random_state=42):\n    train_df, test_df = train_test_split(qa_df, test_size=test_size, random_state=random_state)\n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Building the Vector Retriever for Context Matching","metadata":{}},{"cell_type":"code","source":"def build_retriever(df, model_name=\"all-MiniLM-L6-v2\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = SentenceTransformer(model_name, device=device)\n    texts = df[\"text\"].fillna(\"\").tolist()\n    embeddings = model.encode(texts, show_progress_bar=True, batch_size=16)\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings.astype(np.float32))\n    return model, index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Preparing the QA Dataset for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def prepare_finetuning_dataset(qa_df):\n    def format_example(row):\n        prompt = f\"\"\"\n### Question: {row['question']}\n### Context: {row['context'][:2000]}\n### Instructions:\nAnswer: {row['answer']}\nContext Snippet: {row['context']}\nLocation: {row['context_location']}\n\"\"\"\n        return {\"text\": prompt}\n    \n    dataset = Dataset.from_pandas(qa_df)\n    dataset = dataset.map(format_example)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Fine-Tuning the Mistral Model with LoRA","metadata":{}},{"cell_type":"code","source":"# Fine-tune Mistral model\ndef finetune_mistral(qa_train_df, output_dir=\"mistral_finetuned\"):\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n    \n    # Apply LoRA for efficient fine-tuning\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    \n    # Prepare dataset\n    train_dataset = prepare_finetuning_dataset(qa_train_df)\n    \n    # Tokenize dataset\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"epoch\",\n        evaluation_strategy=\"no\",\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset\n    )\n    \n    trainer.train()\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    return model, tokenizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Retrieving and Generating Answers with the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Retrieve and generate answer with fine-tuned model\ndef retrieve_and_answer(question, paper_df, retriever_model, index, finetuned_model, tokenizer, max_context_length=2000):\n    question_embedding = retriever_model.encode([question])[0]\n    distances, indices = index.search(np.array([question_embedding]).astype(np.float32), k=1)\n    paper_idx = indices[0][0]\n    relevant_paper = paper_df.iloc[paper_idx]\n    context = relevant_paper[\"text\"][:max_context_length]\n    \n    prompt = f\"\"\"\nYou are a highly capable AI assistant trained in reproductive medicine literature. Your task is to reason carefully and answer the question based on the provided academic context.\n\n### Instructions:\n- Read the context below.\n- Infer and construct the most accurate answer possible.\n- Identify and copy the exact sentence or paragraph that supports your answer.\n- Specify its location (e.g., sentence number or paragraph number).\n\n### Format:\nAnswer: <your well-reasoned answer>\nContext Snippet: <exact supporting sentence or paragraph from the context>\nLocation: <where the snippet is found — e.g., \"2nd sentence\", \"3rd paragraph\", etc.>\n\n### Input:\nQuestion: {question}\nContext: {context}\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(\"cuda\")\n    outputs = finetuned_model.generate(**inputs, max_new_tokens=500, temperature=0.2)\n    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Parse the output\n    answer = context_snippet = location = \"Not found\"\n    for line in response_text.split(\"\\n\"):\n        if line.strip().lower().startswith(\"answer:\"):\n            answer = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"context snippet:\"):\n            context_snippet = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"location:\"):\n            location = line.split(\":\", 1)[1].strip()\n    \n    return {\n        \"paper_title\": relevant_paper[\"file\"],\n        \"answer\": answer,\n        \"context_snippet\": context_snippet,\n        \"location\": location,\n        \"paper_idx\": paper_idx\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Evaluating Model Predictions","metadata":{}},{"cell_type":"code","source":"# Evaluate predictions against true labels\ndef evaluate_predictions(test_df, paper_df, retriever_model, index, finetuned_model, tokenizer):\n    results = []\n    exact_match = evaluate.load(\"exact_match\")\n    \n    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating test set\"):\n        question = row[\"question\"]\n        true_labels = {\n            \"paper_title\": row[\"paper\"],\n            \"answer\": row[\"answer\"],\n            \"context_snippet\": row[\"context\"],\n            \"location\": row[\"context_location\"]\n        }\n        \n        pred = retrieve_and_answer(question, paper_df, retriever_model, index, finetuned_model, tokenizer)\n        \n        # Evaluate\n        paper_correct = pred[\"paper_title\"] == true_labels[\"paper_title\"]\n        answer_em = exact_match.compute(predictions=[pred[\"answer\"]], references=[true_labels[\"answer\"]])[\"exact_match\"]\n        context_em = exact_match.compute(predictions=[pred[\"context_snippet\"]], references=[true_labels[\"context_snippet\"]])[\"exact_match\"]\n        location_em = exact_match.compute(predictions=[pred[\"location\"]], references=[str(true_labels[\"location\"])])[\"exact_match\"]\n        \n        results.append({\n            \"question\": question,\n            \"pred_paper\": pred[\"paper_title\"],\n            \"true_paper\": true_labels[\"paper_title\"],\n            \"paper_correct\": paper_correct,\n            \"pred_answer\": pred[\"answer\"],\n            \"true_answer\": true_labels[\"answer\"],\n            \"answer_em\": answer_em,\n            \"pred_context\": pred[\"context_snippet\"],\n            \"true_context\": true_labels[\"context_snippet\"],\n            \"context_em\": context_em,\n            \"pred_location\": pred[\"location\"],\n            \"true_location\": true_labels[\"location\"],\n            \"location_em\": location_em\n        })\n    \n    results_df = pd.DataFrame(results)\n    print(\"\\nEvaluation Summary:\")\n    print(f\"Paper Retrieval Accuracy: {results_df['paper_correct'].mean():.4f}\")\n    print(f\"Answer Exact Match: {results_df['answer_em'].mean():.4f}\")\n    print(f\"Context Exact Match: {results_df['context_em'].mean():.4f}\")\n    print(f\"Location Exact Match: {results_df['location_em'].mean():.4f}\")\n    \n    return results_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Main Execution","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n\n    print(\"Loading paper dataset...\")\n    paper_df = load_papers()\n    print(f\"Loaded {len(paper_df)} papers.\")\n    \n    print(\"Loading QA dataset...\")\n    qa_df = pd.read_csv(\"/kaggle/input/assignmentqa/train_data(2).csv\")  \n    print(f\"Loaded {len(qa_df)} QA pairs.\")\n    \n    print(\"Splitting QA dataset...\")\n    qa_train_df, qa_test_df = split_qa_dataset(qa_df)\n    print(f\"Training set: {len(qa_train_df)} QA pairs\")\n    print(f\"Test set: {len(qa_test_df)} QA pairs\")\n    \n    print(\"Fine-tuning Mistral model...\")\n    finetuned_model, tokenizer = finetune_mistral(qa_train_df)\n   \n    print(\"Building retriever...\")\n    retriever_model, index = build_retriever(paper_df)\n \n    print(\"Evaluating on test set...\")\n    results_df = evaluate_predictions(qa_test_df, paper_df, retriever_model, index, finetuned_model, tokenizer)\n   \n    results_df.to_csv(\"evaluation_results.csv\", index=False)\n    print(\"Results saved to evaluation_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:49:56.338360Z","iopub.execute_input":"2025-05-14T18:49:56.338653Z","iopub.status.idle":"2025-05-14T18:49:56.650806Z","shell.execute_reply.started":"2025-05-14T18:49:56.338632Z","shell.execute_reply":"2025-05-14T18:49:56.649482Z"}},"outputs":[{"name":"stdout","text":"Loading paper dataset...\n","output_type":"stream"},{"name":"stderr","text":"Loading JSON files: 100%|██████████| 43/43 [00:00<00:00, 470.89it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded 43 papers.\nLoading QA dataset...\nLoaded 215 QA pairs.\nSplitting QA dataset...\nTraining set: 172 QA pairs\nTest set: 43 QA pairs\nFine-tuning Mistral model...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1449\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    472\u001b[0m             )\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-6824e5d4-7477b09b380dcf30606709fa;6c4a3b2f-80bc-4956-8feb-577252521548)\n\n403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\nCannot access content at: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nMake sure your token has the correct permissions.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1647\u001b[0m             \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2914514102.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Fine-tune Mistral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fine-tuning Mistral model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfinetuned_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_mistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# # Build retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/712036078.py\u001b[0m in \u001b[0;36mfinetune_mistral\u001b[0;34m(qa_train_df, output_dir)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinetune_mistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistral_finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    967\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;31m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 raise OSError(\n\u001b[0m\u001b[1;32m    492\u001b[0m                     \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0;34mf\" cached files.\\nCheckout your internet connection or see how to run the library in offline mode at\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."],"ename":"OSError","evalue":"We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom sklearn.model_selection import train_test_split\n\n# Load JSON papers\ndef load_papers(directory=\"/kaggle/input/assignementdataset\"):\n    directory_path = Path(directory)\n    if not directory_path.exists():\n        raise FileNotFoundError(f\"Directory {directory} does not exist. Please check the path.\")\n    \n    json_files = [f for f in directory_path.glob(\"*.json\") if f.is_file()]\n    if not json_files:\n        raise ValueError(f\"No JSON files found in {directory}. Please ensure the directory contains valid JSON files.\")\n    \n    all_data = []\n    for file in tqdm(json_files, desc=\"Loading JSON files\"):\n        with open(file, 'r') as f:\n            try:\n                doc = json.load(f)\n                text = extract_text_from_json(doc)\n                if not text.strip():\n                    print(f\"Warning: Empty text extracted from {file.name}\")\n                    continue\n                all_data.append({\"file\": file.name, \"text\": text})\n            except json.JSONDecodeError as e:\n                print(f\"Error parsing {file.name}: {e}\")\n                continue\n            except Exception as e:\n                print(f\"Unexpected error processing {file.name}: {e}\")\n                continue\n    \n    papers_df = pd.DataFrame(all_data)\n    if papers_df.empty:\n        raise ValueError(\"No valid papers loaded. Check JSON files for content or parsing errors.\")\n    \n    print(f\"Loaded {len(papers_df)} papers successfully.\")\n    return papers_df\n\ndef extract_text_from_json(doc):\n    if \"body_text\" in doc:\n        return \" \".join([section[\"text\"] for section in doc[\"body_text\"] if \"text\" in section])\n    elif isinstance(doc, dict):\n        return \" \".join(str(v) for v in doc.values() if isinstance(v, str))\n    return \"\"\n\n# Load and prepare QA dataset\ndef load_qa_dataset(csv_path=\"/kaggle/input/assignmentqa/train_data(2).csv\"):\n    if not Path(csv_path).exists():\n        raise FileNotFoundError(f\"CSV file {csv_path} not found.\")\n    \n    df = pd.read_csv(csv_path)\n    required_columns = [\"question\", \"answer\", \"context\", \"context_location\", \"paper\"]\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"CSV missing required columns: {missing_columns}\")\n    \n    if df.empty:\n        raise ValueError(\"CSV file is empty. Please provide a valid dataset.\")\n    \n    print(f\"Loaded {len(df)} QA pairs from {csv_path}.\")\n    return df\n\n# Build vector retriever\ndef build_retriever(df, model_name=\"all-MiniLM-L6-v2\"):\n    if df.empty:\n        raise ValueError(\"Papers DataFrame is empty. Cannot build retriever.\")\n    \n    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    texts = df[\"text\"].fillna(\"\").tolist()\n    if not any(texts):\n        raise ValueError(\"All paper texts are empty. Cannot build embeddings.\")\n    \n    embeddings = model.encode(texts, show_progress_bar=True, batch_size=16)\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings.astype(np.float32))\n    return model, index\n\n# Prepare training dataset\ndef prepare_training_data(qa_df, papers_df, max_context_length=2000):\n    data = []\n    skipped_rows = []\n    \n    # Normalize filenames for matching\n    papers_df[\"file_normalized\"] = papers_df[\"file\"].str.replace(r\"\\.json$\", \"\", regex=True)\n    qa_df[\"paper_normalized\"] = qa_df[\"paper\"].str.replace(r\"\\.json$\", \"\", regex=True)\n    \n    for idx, row in tqdm(qa_df.iterrows(), desc=\"Preparing training data\", total=len(qa_df)):\n        question = row[\"question\"]\n        answer = row[\"answer\"]\n        context_snippet = row[\"context\"]\n        location = row[\"context_location\"]\n        paper_name = row[\"paper_normalized\"]\n        \n        # Retrieve full paper text\n        paper_row = papers_df[papers_df[\"file_normalized\"] == paper_name]\n        if paper_row.empty:\n            skipped_rows.append((idx, paper_name, \"Paper not found in papers_df\"))\n            continue\n        \n        context = paper_row.iloc[0][\"text\"][:max_context_length]\n        if not context.strip():\n            skipped_rows.append((idx, paper_name, \"Empty paper text\"))\n            continue\n        \n        # Create prompt\n        prompt = f\"\"\"\nYou are a highly capable AI assistant trained in reproductive medicine literature. Your task is to answer the question based on the provided academic context and identify the supporting context snippet and its location.\n\n### Input:\nQuestion: {question}\nContext: {context}\n\n### Output:\nAnswer: {answer}\nContext Snippet: {context_snippet}\nLocation: {location}\n\"\"\"\n        data.append({\n            \"prompt\": prompt,\n            \"output\": f\"Answer: {answer}\\nContext Snippet: {context_snippet}\\nLocation: {location}\"\n        })\n    \n    # Log skipped rows\n    if skipped_rows:\n        print(f\"Skipped {len(skipped_rows)} rows due to issues:\")\n        for idx, paper_name, reason in skipped_rows[:10]:  # Limit to first 10 for brevity\n            print(f\"Row {idx}, Paper: {paper_name}, Reason: {reason}\")\n        if len(skipped_rows) > 10:\n            print(f\"... and {len(skipped_rows) - 10} more skipped rows.\")\n    \n    if not data:\n        raise ValueError(\"No valid training examples created. Check paper filenames and data integrity.\")\n    \n    print(f\"Created {len(data)} valid training examples.\")\n    return Dataset.from_list(data)\n\n# Split dataset\ndef split_dataset(dataset, test_size=0.2, random_state=42):\n    if len(dataset) == 0:\n        raise ValueError(\"Dataset is empty. Cannot perform train-test split.\")\n    \n    train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=random_state)\n    print(f\"Split dataset: {len(train_data)} training, {len(val_data)} validation examples.\")\n    \n    if not train_data or not val_data:\n        raise ValueError(\"Train or validation set is empty after split. Adjust test_size or check dataset.\")\n    \n    return Dataset.from_list(train_data), Dataset.from_list(val_data)\n\n# Fine-tune Mistral 7B\ndef fine_tune_mistral(train_dataset, val_dataset, model_name=\"mistralai/Mixtral-7B-Instruct-v0.1\", output_dir=\"./mistral_finetuned\"):\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n    # Apply LoRA\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n\n    # Tokenize dataset\n    def tokenize_function(examples):\n        inputs = tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=2048)\n        outputs = tokenizer(examples[\"output\"], padding=\"max_length\", truncation=True, max_length=512)\n        inputs[\"labels\"] = outputs[\"input_ids\"]\n        return inputs\n\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=2,\n        evaluation_strategy=\"steps\",\n        eval_steps=50,\n        report_to=\"none\"\n    )\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the fine-tuned model\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    return model, tokenizer\n\n# Inference with fine-tuned model\ndef retrieve_and_answer_finetuned(question, papers_df, retriever_model, index, finetuned_model, tokenizer, max_context_length=2000):\n    # Retrieve relevant paper\n    question_embedding = retriever_model.encode([question])[0]\n    distances, indices = index.search(np.array([question_embedding]).astype(np.float32), k=1)\n    paper_idx = indices[0][0]\n    relevant_paper = papers_df.iloc[paper_idx]\n    context = relevant_paper[\"text\"][:max_context_length]\n\n    # Create prompt\n    prompt = f\"\"\"\nYou are a highly capable AI assistant trained in reproductive medicine literature. Your task is to answer the question based on the provided academic context and identify the supporting context snippet and its location.\n\n### Input:\nQuestion: {question}\nContext: {context}\n\n### Output:\nAnswer: \nContext Snippet: \nLocation: \n\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(\"cuda\")\n    outputs = finetuned_model.generate(**inputs, max_new_tokens=500, temperature=0.2)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Parse response\n    answer = context_snippet = location = \"Not found\"\n    for line in response.split(\"\\n\"):\n        if line.strip().lower().startswith(\"answer:\"):\n            answer = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"context snippet:\"):\n            context_snippet = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"location:\"):\n            location = line.split(\":\", 1)[1].strip()\n\n    return {\n        \"paper_title\": relevant_paper[\"file\"],\n        \"answer\": answer,\n        \"context_snippet\": context_snippet,\n        \"location\": location,\n        \"paper_idx\": paper_idx\n    }\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"Loading papers...\")\n    papers_df = load_papers()\n    \n    print(\"Loading QA dataset...\")\n    qa_df = load_qa_dataset()\n    \n    print(\"Building retriever...\")\n    retriever_model, index = build_retriever(papers_df)\n    \n    print(\"Preparing training data...\")\n    dataset = prepare_training_data(qa_df, papers_df)\n    train_dataset, val_dataset = split_dataset(dataset)\n    \n    print(\"Fine-tuning Mistral 7B...\")\n    finetuned_model, finetuned_tokenizer = fine_tune_mistral(train_dataset, val_dataset)\n    \n    print(\"Testing fine-tuned model...\")\n    custom_question = input(\"\\nEnter your question: \")\n    result = retrieve_and_answer_finetuned(custom_question, papers_df, retriever_model, index, finetuned_model, finetuned_tokenizer)\n    \n    print(\"\\nResults:\")\n    print(f\"Most Relevant Paper: {result['paper_title']}\")\n    print(f\"Answer: {result['answer']}\")\n    print(f\"Context Snippet: {result['context_snippet']}\")\n    print(f\"Location: {result['location']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:42:50.237004Z","iopub.execute_input":"2025-05-14T18:42:50.237888Z","iopub.status.idle":"2025-05-14T18:42:51.309017Z","shell.execute_reply.started":"2025-05-14T18:42:50.237853Z","shell.execute_reply":"2025-05-14T18:42:51.307847Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"Loading papers...\n","output_type":"stream"},{"name":"stderr","text":"Loading JSON files: 100%|██████████| 43/43 [00:00<00:00, 344.26it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded 43 papers successfully.\nLoading QA dataset...\nLoaded 215 QA pairs from /kaggle/input/assignmentqa/train_data(2).csv.\nBuilding retriever...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64593aec9ea4431bbe53dd85c01782dd"}},"metadata":{}},{"name":"stdout","text":"Preparing training data...\n","output_type":"stream"},{"name":"stderr","text":"Preparing training data: 100%|██████████| 215/215 [00:00<00:00, 2913.23it/s]","output_type":"stream"},{"name":"stdout","text":"Skipped 215 rows due to issues:\nRow 0, Paper: Awareness-knowledge-and-misconceptions-of-adoles, Reason: Paper not found in papers_df\nRow 1, Paper: Awareness-knowledge-and-misconceptions-of-adoles, Reason: Paper not found in papers_df\nRow 2, Paper: Awareness-knowledge-and-misconceptions-of-adoles, Reason: Paper not found in papers_df\nRow 3, Paper: Awareness-knowledge-and-misconceptions-of-adoles, Reason: Paper not found in papers_df\nRow 4, Paper: Awareness-knowledge-and-misconceptions-of-adoles, Reason: Paper not found in papers_df\nRow 5, Paper: A-novel-approach-using-vaginal-natural-orifice-tra, Reason: Paper not found in papers_df\nRow 6, Paper: A-novel-approach-using-vaginal-natural-orifice-tra, Reason: Paper not found in papers_df\nRow 7, Paper: A-novel-approach-using-vaginal-natural-orifice-tra, Reason: Paper not found in papers_df\nRow 8, Paper: A-novel-approach-using-vaginal-natural-orifice-tra, Reason: Paper not found in papers_df\nRow 9, Paper: A-novel-approach-using-vaginal-natural-orifice-tra, Reason: Paper not found in papers_df\n... and 205 more skipped rows.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3280595644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpapers_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3280595644.py\u001b[0m in \u001b[0;36mprepare_training_data\u001b[0;34m(qa_df, papers_df, max_context_length)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid training examples created. Check paper filenames and data integrity.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Created {len(data)} valid training examples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No valid training examples created. Check paper filenames and data integrity."],"ename":"ValueError","evalue":"No valid training examples created. Check paper filenames and data integrity.","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}