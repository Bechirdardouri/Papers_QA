{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f27082",
   "metadata": {},
   "source": [
    "# Papers QA: Production Pipeline\n",
    "\n",
    "This notebook demonstrates the complete Papers QA pipeline with professional-grade code and best practices.\n",
    "\n",
    "## Sections:\n",
    "1. Setup & Configuration\n",
    "2. Data Loading & Processing\n",
    "3. Embedding & Indexing\n",
    "4. QA Generation\n",
    "5. Retrieval & Inference\n",
    "6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346c04e",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import from papers_qa\n",
    "from papers_qa import (\n",
    "    DataLoader,\n",
    "    DataProcessor,\n",
    "    RetrieverPipeline,\n",
    "    QAGenerator,\n",
    "    QAEvaluator,\n",
    "    configure_logging,\n",
    "    get_logger,\n",
    "    get_settings,\n",
    ")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "configure_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Get settings\n",
    "settings = get_settings()\n",
    "\n",
    "print(f\"Environment: {settings.environment}\")\n",
    "print(f\"Log Level: {settings.log_level}\")\n",
    "print(f\"Embedding Model: {settings.model.embedding_model}\")\n",
    "print(f\"Generation Model: {settings.model.generation_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eddd6b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e24ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = DataLoader()\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Load documents from raw data directory\n",
    "documents = loader.load_documents(settings.data.input_dir)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\nFirst document structure:\")\n",
    "    print(json.dumps(documents[0], indent=2, default=str)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process text\n",
    "texts = []\n",
    "for doc in documents:\n",
    "    if isinstance(doc, dict):\n",
    "        text = processor.extract_text_from_doc(doc)\n",
    "    else:\n",
    "        text = str(doc)\n",
    "    texts.append(text)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"Total documents: {len(texts)}\")\n",
    "print(f\"Average text length: {sum(len(t.split()) for t in texts) / len(texts):.0f} words\")\n",
    "print(f\"Min/Max length: {min(len(t.split()) for t in texts)}/{max(len(t.split()) for t in texts)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee710db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split long documents into chunks\n",
    "chunks = []\n",
    "for text in texts:\n",
    "    text_chunks = processor.split_text(\n",
    "        text,\n",
    "        chunk_size=settings.data.chunk_size,\n",
    "        overlap=settings.data.chunk_overlap\n",
    "    )\n",
    "    chunks.extend(text_chunks)\n",
    "\n",
    "print(f\"Total chunks after splitting: {len(chunks)}\")\n",
    "print(f\"Sample chunk length: {len(chunks[0].split())} words\" if chunks else \"No chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee06c23",
   "metadata": {},
   "source": [
    "## 3. Embedding & Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aea217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever pipeline\n",
    "retriever = RetrieverPipeline()\n",
    "\n",
    "# Index documents (using chunks)\n",
    "print(f\"Indexing {len(chunks)} chunks...\")\n",
    "retriever.index_documents(chunks)\n",
    "print(f\"Indexing complete!\")\n",
    "\n",
    "# Save index\n",
    "print(f\"Saving index...\")\n",
    "retriever.save()\n",
    "print(f\"Index saved to {settings.data.cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d03134",
   "metadata": {},
   "source": [
    "## 4. QA Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QA generator\n",
    "generator = QAGenerator()\n",
    "\n",
    "# Generate QA pairs from first few chunks\n",
    "qa_pairs_list = []\n",
    "for i, chunk in enumerate(chunks[:3]):  # Process first 3 chunks as example\n",
    "    print(f\"Generating QA pairs for chunk {i+1}...\")\n",
    "    try:\n",
    "        qa_pairs = generator.generate_qa_pairs(chunk)\n",
    "        qa_pairs_list.append(qa_pairs)\n",
    "        print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating QA pairs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA dataset\n",
    "qa_records = []\n",
    "for chunk_idx, qa_pairs in enumerate(qa_pairs_list):\n",
    "    for qa_pair in qa_pairs:\n",
    "        qa_records.append({\n",
    "            'question': qa_pair.get('question', ''),\n",
    "            'answer': qa_pair.get('answer', ''),\n",
    "            'context': chunks[chunk_idx],\n",
    "            'chunk_id': chunk_idx\n",
    "        })\n",
    "\n",
    "qa_df = pd.DataFrame(qa_records)\n",
    "\n",
    "print(f\"\\nGenerated QA Dataset:\")\n",
    "print(f\"Total QA pairs: {len(qa_df)}\")\n",
    "print(f\"\\nDataset preview:\")\n",
    "print(qa_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QA dataset\n",
    "output_path = settings.data.output_dir / 'generated_qa_pairs.csv'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "qa_df.to_csv(output_path, index=False)\n",
    "print(f\"QA dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6334b",
   "metadata": {},
   "source": [
    "## 5. Retrieval & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved index\n",
    "retriever = RetrieverPipeline()\n",
    "retriever.load()\n",
    "print(f\"Index loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "test_queries = [\n",
    "    \"What is the main topic of this paper?\",\n",
    "    \"What are the key findings?\",\n",
    "    \"What methods were used?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = retriever.retrieve(query, k=3)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Similarity Score: {score:.4f}\")\n",
    "        print(f\"Document: {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf15c2",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = QAEvaluator()\n",
    "\n",
    "# Example evaluation\n",
    "if len(qa_df) > 0:\n",
    "    # Use first QA pair as example\n",
    "    reference_answer = qa_df.iloc[0]['answer']\n",
    "    # For demo, use a slightly different answer\n",
    "    hypothesis_answer = reference_answer[:100] + \"...\" if len(reference_answer) > 100 else reference_answer\n",
    "    \n",
    "    print(f\"Reference: {reference_answer[:100]}...\")\n",
    "    print(f\"\\nHypothesis: {hypothesis_answer}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    metrics = evaluator.evaluate_answer(reference_answer, hypothesis_answer)\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation\n",
    "if len(qa_df) > 1:\n",
    "    from papers_qa import BatchEvaluator\n",
    "    \n",
    "    batch_eval = BatchEvaluator()\n",
    "    \n",
    "    # Evaluate first few answers\n",
    "    references = qa_df['answer'].head(5).tolist()\n",
    "    # For demo, slightly modify predictions\n",
    "    predictions = [r[:80] if len(r) > 80 else r for r in references]\n",
    "    \n",
    "    batch_metrics = batch_eval.evaluate_qa_pairs(references, predictions)\n",
    "    \n",
    "    print(\"\\nBatch Evaluation Results:\")\n",
    "    for key, value in batch_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21da824",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete Papers QA pipeline:\n",
    "\n",
    "1. **Setup**: Configuration management with Pydantic\n",
    "2. **Data Loading**: Load and process documents\n",
    "3. **Preprocessing**: Text cleaning and chunking\n",
    "4. **Embeddings**: Create vector index with FAISS\n",
    "5. **QA Generation**: Generate question-answer pairs\n",
    "6. **Retrieval**: Retrieve relevant documents\n",
    "7. **Evaluation**: Comprehensive metrics (BLEU, ROUGE, semantic similarity)\n",
    "\n",
    "### Next Steps:\n",
    "- Use CLI for production workflows: `papers-qa generate --input data/raw`\n",
    "- Deploy with Docker: `docker-compose up`\n",
    "- Run tests: `pytest tests/`\n",
    "- See SETUP_GUIDE.md for more details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
