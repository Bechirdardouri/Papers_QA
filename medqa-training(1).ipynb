{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10337673,"sourceType":"datasetVersion","datasetId":6401265},{"sourceId":11796267,"sourceType":"datasetVersion","datasetId":7407516}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_API_KEY\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"login(secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Imports and Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport glob\nimport json\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nimport evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Loading and Extracting Data from JSON Papers","metadata":{}},{"cell_type":"code","source":"\ndef load_papers(directory=\"/kaggle/input/assignementdataset\"):\n    json_files = [f for f in Path(directory).glob(\"*.json\") if f.is_file()]\n    all_data = []\n    for file in tqdm(json_files, desc=\"Loading JSON files\"):\n        with open(file, 'r') as f:\n            try:\n                doc = json.load(f)\n                text = extract_text_from_json(doc)\n                all_data.append({\"file\": file.name, \"text\": text})\n            except json.JSONDecodeError as e:\n                print(f\"Error parsing {file.name}: {e}\")\n                continue\n    return pd.DataFrame(all_data)\n\ndef extract_text_from_json(doc):\n    if \"body_text\" in doc:\n        return \" \".join([section[\"text\"] for section in doc[\"body_text\"] if \"text\" in section])\n    elif isinstance(doc, dict):\n        return \" \".join(str(v) for v in doc.values() if isinstance(v, str))\n    return \"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Splitting the QA Dataset into Training and Testing","metadata":{}},{"cell_type":"code","source":"def split_qa_dataset(qa_df, test_size=0.2, random_state=42):\n    train_df, test_df = train_test_split(qa_df, test_size=test_size, random_state=random_state)\n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Building the Vector Retriever for Context Matching","metadata":{}},{"cell_type":"code","source":"def build_retriever(df, model_name=\"all-MiniLM-L6-v2\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = SentenceTransformer(model_name, device=device)\n    texts = df[\"text\"].fillna(\"\").tolist()\n    embeddings = model.encode(texts, show_progress_bar=True, batch_size=16)\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings.astype(np.float32))\n    return model, index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Preparing the QA Dataset for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def prepare_finetuning_dataset(qa_df):\n    def format_example(row):\n        prompt = f\"\"\"\n### Question: {row['question']}\n### Context: {row['context'][:2000]}\n### Instructions:\nAnswer: {row['answer']}\nContext Snippet: {row['context']}\nLocation: {row['context_location']}\n\"\"\"\n        return {\"text\": prompt}\n    \n    dataset = Dataset.from_pandas(qa_df)\n    dataset = dataset.map(format_example)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Fine-Tuning the Mistral Model with LoRA","metadata":{}},{"cell_type":"code","source":"# Fine-tune Mistral model\ndef finetune_mistral(qa_train_df, output_dir=\"mistral_finetuned\"):\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n    \n    # Apply LoRA for efficient fine-tuning\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    \n    # Prepare dataset\n    train_dataset = prepare_finetuning_dataset(qa_train_df)\n    \n    # Tokenize dataset\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"epoch\",\n        evaluation_strategy=\"no\",\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset\n    )\n    \n    trainer.train()\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    return model, tokenizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Retrieving and Generating Answers with the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# Retrieve and generate answer with fine-tuned model\ndef retrieve_and_answer(question, paper_df, retriever_model, index, finetuned_model, tokenizer, max_context_length=2000):\n    question_embedding = retriever_model.encode([question])[0]\n    distances, indices = index.search(np.array([question_embedding]).astype(np.float32), k=1)\n    paper_idx = indices[0][0]\n    relevant_paper = paper_df.iloc[paper_idx]\n    context = relevant_paper[\"text\"][:max_context_length]\n    \n    prompt = f\"\"\"\nYou are a highly capable AI assistant trained in reproductive medicine literature. Your task is to reason carefully and answer the question based on the provided academic context.\n\n### Instructions:\n- Read the context below.\n- Infer and construct the most accurate answer possible.\n- Identify and copy the exact sentence or paragraph that supports your answer.\n- Specify its location (e.g., sentence number or paragraph number).\n\n### Format:\nAnswer: <your well-reasoned answer>\nContext Snippet: <exact supporting sentence or paragraph from the context>\nLocation: <where the snippet is found â€” e.g., \"2nd sentence\", \"3rd paragraph\", etc.>\n\n### Input:\nQuestion: {question}\nContext: {context}\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(\"cuda\")\n    outputs = finetuned_model.generate(**inputs, max_new_tokens=500, temperature=0.2)\n    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Parse the output\n    answer = context_snippet = location = \"Not found\"\n    for line in response_text.split(\"\\n\"):\n        if line.strip().lower().startswith(\"answer:\"):\n            answer = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"context snippet:\"):\n            context_snippet = line.split(\":\", 1)[1].strip()\n        elif line.strip().lower().startswith(\"location:\"):\n            location = line.split(\":\", 1)[1].strip()\n    \n    return {\n        \"paper_title\": relevant_paper[\"file\"],\n        \"answer\": answer,\n        \"context_snippet\": context_snippet,\n        \"location\": location,\n        \"paper_idx\": paper_idx\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Evaluating Model Predictions","metadata":{}},{"cell_type":"code","source":"# Evaluate predictions against true labels\ndef evaluate_predictions(test_df, paper_df, retriever_model, index, finetuned_model, tokenizer):\n    results = []\n    exact_match = evaluate.load(\"exact_match\")\n    \n    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating test set\"):\n        question = row[\"question\"]\n        true_labels = {\n            \"paper_title\": row[\"paper\"],\n            \"answer\": row[\"answer\"],\n            \"context_snippet\": row[\"context\"],\n            \"location\": row[\"context_location\"]\n        }\n        \n        pred = retrieve_and_answer(question, paper_df, retriever_model, index, finetuned_model, tokenizer)\n        \n        # Evaluate\n        paper_correct = pred[\"paper_title\"] == true_labels[\"paper_title\"]\n        answer_em = exact_match.compute(predictions=[pred[\"answer\"]], references=[true_labels[\"answer\"]])[\"exact_match\"]\n        context_em = exact_match.compute(predictions=[pred[\"context_snippet\"]], references=[true_labels[\"context_snippet\"]])[\"exact_match\"]\n        location_em = exact_match.compute(predictions=[pred[\"location\"]], references=[str(true_labels[\"location\"])])[\"exact_match\"]\n        \n        results.append({\n            \"question\": question,\n            \"pred_paper\": pred[\"paper_title\"],\n            \"true_paper\": true_labels[\"paper_title\"],\n            \"paper_correct\": paper_correct,\n            \"pred_answer\": pred[\"answer\"],\n            \"true_answer\": true_labels[\"answer\"],\n            \"answer_em\": answer_em,\n            \"pred_context\": pred[\"context_snippet\"],\n            \"true_context\": true_labels[\"context_snippet\"],\n            \"context_em\": context_em,\n            \"pred_location\": pred[\"location\"],\n            \"true_location\": true_labels[\"location\"],\n            \"location_em\": location_em\n        })\n    \n    results_df = pd.DataFrame(results)\n    print(\"\\nEvaluation Summary:\")\n    print(f\"Paper Retrieval Accuracy: {results_df['paper_correct'].mean():.4f}\")\n    print(f\"Answer Exact Match: {results_df['answer_em'].mean():.4f}\")\n    print(f\"Context Exact Match: {results_df['context_em'].mean():.4f}\")\n    print(f\"Location Exact Match: {results_df['location_em'].mean():.4f}\")\n    \n    return results_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Main Execution","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n\n    print(\"Loading paper dataset...\")\n    paper_df = load_papers()\n    print(f\"Loaded {len(paper_df)} papers.\")\n    \n    print(\"Loading QA dataset...\")\n    qa_df = pd.read_csv(\"/kaggle/input/assignmentqa/train_data(2).csv\")  \n    print(f\"Loaded {len(qa_df)} QA pairs.\")\n    \n    print(\"Splitting QA dataset...\")\n    qa_train_df, qa_test_df = split_qa_dataset(qa_df)\n    print(f\"Training set: {len(qa_train_df)} QA pairs\")\n    print(f\"Test set: {len(qa_test_df)} QA pairs\")\n    \n    print(\"Fine-tuning Mistral model...\")\n    finetuned_model, tokenizer = finetune_mistral(qa_train_df)\n   \n    print(\"Building retriever...\")\n    retriever_model, index = build_retriever(paper_df)\n \n    print(\"Evaluating on test set...\")\n    results_df = evaluate_predictions(qa_test_df, paper_df, retriever_model, index, finetuned_model, tokenizer)\n   \n    results_df.to_csv(\"evaluation_results.csv\", index=False)\n    print(\"Results saved to evaluation_results.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}